{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiran-kusuma/A-Distributed-Computation-on-shared-resouces/blob/main/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aY2_kdqJ3-n",
        "outputId": "9b3d74ed-ed3c-47a8-9fd4-d5281f6dc150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: globalwindmastsinventorydata_nov_9_2020.csv\n",
            "Processing file: california_housing_train.csv\n",
            "Metadata has been saved to /content/database_metadata.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Specify the folder path containing the CSV files\n",
        "folder_path = '/content/sample_data'\n",
        "\n",
        "# Create a dictionary to store columns and their types for each CSV\n",
        "tables_info = {}\n",
        "\n",
        "# Create a dictionary to store relationships between files and columns\n",
        "relationships = {}\n",
        "\n",
        "# Create a dictionary to store the metadata in JSON format\n",
        "database_metadata = {}\n",
        "\n",
        "# Step 1: Identify potential relationships first\n",
        "for filename in os.listdir(folder_path):\n",
        "    full_file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "    # Check if it's a CSV file (based on the extension)\n",
        "    if os.path.isfile(full_file_path) and filename.endswith('.csv'):\n",
        "        print(f\"Processing file: {filename}\")\n",
        "        # Load the CSV file using pandas\n",
        "        df = pd.read_csv(full_file_path, encoding='latin-1')  # or 'utf-16', 'ISO-8859-1', etc.\n",
        "\n",
        "        # Get column names and types\n",
        "        columns_and_types = df.dtypes.to_dict()\n",
        "\n",
        "        # Store the table name, columns, and their data types\n",
        "        tables_info[filename] = columns_and_types\n",
        "\n",
        "# Compare columns across all files to find common columns\n",
        "for file1, columns1 in tables_info.items():\n",
        "    for file2, columns2 in tables_info.items():\n",
        "        if file1 != file2:\n",
        "            common_columns = set(columns1).intersection(columns2)\n",
        "\n",
        "            if common_columns:\n",
        "                for common_col in common_columns:\n",
        "                    # Load the data for the common column\n",
        "                    df1 = pd.read_csv(os.path.join(folder_path, file1), engine='python')\n",
        "                    df2 = pd.read_csv(os.path.join(folder_path, file2), engine='python')\n",
        "\n",
        "                    # Check if any values in the common column match between the two files\n",
        "                    matching_values = set(df1[common_col]).intersection(set(df2[common_col]))\n",
        "\n",
        "                    if matching_values:\n",
        "                        # Store the relationship for later use in the schema\n",
        "                        if file1 not in relationships:\n",
        "                            relationships[file1] = {}\n",
        "                        if file2 not in relationships:\n",
        "                            relationships[file2] = {}\n",
        "\n",
        "                        relationships[file1][common_col] = (file2, common_col)\n",
        "                        relationships[file2][common_col] = (file1, common_col)\n",
        "\n",
        "# Step 2: Build the JSON metadata\n",
        "for filename, columns_and_types in tables_info.items():\n",
        "    table_metadata = {\n",
        "        \"columns\": []\n",
        "    }\n",
        "\n",
        "    for column, dtype in columns_and_types.items():\n",
        "        column_metadata = {\n",
        "            \"name\": column,\n",
        "            \"type\": str(dtype)\n",
        "        }\n",
        "\n",
        "        # Check if there is a relationship for this column\n",
        "        if column in relationships.get(filename, {}):\n",
        "            related_table, related_column = relationships[filename][column]\n",
        "            column_metadata[\"references\"] = {\n",
        "                \"table\": related_table,\n",
        "                \"column\": related_column\n",
        "            }\n",
        "\n",
        "        table_metadata[\"columns\"].append(column_metadata)\n",
        "\n",
        "    # Add the table metadata to the database\n",
        "    database_metadata[filename] = table_metadata\n",
        "\n",
        "# Save the metadata to a JSON file\n",
        "json_schema_file_path = '/content/database_metadata.json'\n",
        "with open(json_schema_file_path, 'w') as json_file:\n",
        "    json.dump(database_metadata, json_file, indent=4)\n",
        "\n",
        "print(f\"Metadata has been saved to {json_schema_file_path}\")\n"
      ]
    },
    {
      "source": [
        "df = pd.read_csv(full_file_path, encoding='latin-1')  # or 'utf-16', 'ISO-8859-1', etc."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "KK6XNNZ-EABQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMgw5Z7MLSa7",
        "outputId": "4a6d610e-55d6-43e0-c831-728bebb2eba1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'WDIfootnote': 788559, 'WDIcountry-series': 8237, 'WDICSV': 397936, 'WDICountry': 265, 'WDIseries-time': 148, 'WDISeries': 1453}\n",
            "[10, 1, 6, 1, 1, 1]\n",
            "Table: WDIfootnote | Shard 0 | Rows Allocated: 78856\n",
            "Table: WDIfootnote | Shard 1 | Rows Allocated: 78856\n",
            "Table: WDIfootnote | Shard 2 | Rows Allocated: 78856\n",
            "Table: WDIfootnote | Shard 3 | Rows Allocated: 78856\n",
            "Table: WDIfootnote | Shard 4 | Rows Allocated: 78856\n",
            "Table: WDIfootnote | Shard 5 | Rows Allocated: 78856\n",
            "Table: WDIfootnote | Shard 6 | Rows Allocated: 78856\n",
            "Table: WDIfootnote | Shard 7 | Rows Allocated: 78856\n",
            "Table: WDIfootnote | Shard 8 | Rows Allocated: 78856\n",
            "Table: WDIfootnote | Shard 9 | Rows Allocated: 78855\n",
            "Table: WDIcountry-series | Shard 10 | Rows Allocated: 8237\n",
            "Table: WDICSV | Shard 11 | Rows Allocated: 66323\n",
            "Table: WDICSV | Shard 12 | Rows Allocated: 66323\n",
            "Table: WDICSV | Shard 13 | Rows Allocated: 66323\n",
            "Table: WDICSV | Shard 14 | Rows Allocated: 66323\n",
            "Table: WDICSV | Shard 15 | Rows Allocated: 66322\n",
            "Table: WDICSV | Shard 16 | Rows Allocated: 66322\n",
            "Table: WDICountry | Shard 17 | Rows Allocated: 265\n",
            "Table: WDIseries-time | Shard 18 | Rows Allocated: 148\n",
            "Table: WDISeries | Shard 19 | Rows Allocated: 1453\n",
            "Shard 0 saved to /content/sharded_database/shard_0.csv\n",
            "Shard 1 saved to /content/sharded_database/shard_1.csv\n",
            "Shard 2 saved to /content/sharded_database/shard_2.csv\n",
            "Shard 3 saved to /content/sharded_database/shard_3.csv\n",
            "Shard 4 saved to /content/sharded_database/shard_4.csv\n",
            "Shard 5 saved to /content/sharded_database/shard_5.csv\n",
            "Shard 6 saved to /content/sharded_database/shard_6.csv\n",
            "Shard 7 saved to /content/sharded_database/shard_7.csv\n",
            "Shard 8 saved to /content/sharded_database/shard_8.csv\n",
            "Shard 9 saved to /content/sharded_database/shard_9.csv\n",
            "Shard 10 saved to /content/sharded_database/shard_10.csv\n",
            "Shard 11 saved to /content/sharded_database/shard_11.csv\n",
            "Shard 12 saved to /content/sharded_database/shard_12.csv\n",
            "Shard 13 saved to /content/sharded_database/shard_13.csv\n",
            "Shard 14 saved to /content/sharded_database/shard_14.csv\n",
            "Shard 15 saved to /content/sharded_database/shard_15.csv\n",
            "Shard 16 saved to /content/sharded_database/shard_16.csv\n",
            "Shard 17 saved to /content/sharded_database/shard_17.csv\n",
            "Shard 18 saved to /content/sharded_database/shard_18.csv\n",
            "Shard 19 saved to /content/sharded_database/shard_19.csv\n",
            "Metadata saved to /content/sharded_database/shard_metadata.json\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "def shard_database(input_folder, output_folder, shard_count):\n",
        "    # Step 1: Create output folder if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Step 2: Load all tables (CSV files) into a list\n",
        "    tables = []\n",
        "    table_names = []\n",
        "    table_row_counts = {}\n",
        "\n",
        "    # Load all tables and calculate total rows for each table\n",
        "    for file_path in Path(input_folder).glob('*.csv'):\n",
        "        df = pd.read_csv(file_path)\n",
        "        table_name = file_path.stem\n",
        "        tables.append(df)\n",
        "        table_names.append(table_name)\n",
        "        table_row_counts[table_name] = len(df)\n",
        "\n",
        "    print(table_row_counts)\n",
        "\n",
        "    # Step 3: Calculate rows per shard\n",
        "    k = len(table_names)\n",
        "    N_values = [1] * k  # Initial distribution of rows\n",
        "    N = shard_count  # Total number of shards\n",
        "\n",
        "    # While the sum of N_values is not equal to N, increment the one that maximizes the objective\n",
        "    current_sum = sum(N_values)\n",
        "    while current_sum < N:\n",
        "        # Calculate the current objective values for each table\n",
        "        obj_values = [table_row_counts[table_names[i]] / N_values[i] for i in range(k)]\n",
        "\n",
        "        # Find the index with the maximum value of n[i] / N_values[i]\n",
        "        max_index = np.argmax(obj_values)\n",
        "\n",
        "        # Increment the corresponding N_value\n",
        "        N_values[max_index] += 1\n",
        "        current_sum += 1  # Update the total sum\n",
        "    print(N_values)\n",
        "\n",
        "    # Step 4: Initialize variables for row distribution across shards\n",
        "    shard_data = {i: [] for i in range(shard_count)}  # To store data for each shard\n",
        "    table_shard_allocation = {}\n",
        "    table_metadata = {}  # Dictionary to store metadata for each table\n",
        "\n",
        "    # Step 5: Distribute rows into shards (with careful allocation)\n",
        "    current_shard_index = 0  # Initialize shard index to 0 for the first table\n",
        "    for i, table in enumerate(tables):\n",
        "        table_name = table_names[i]\n",
        "        table_row_count = table_row_counts[table_name]\n",
        "\n",
        "        # Calculate how many rows to allocate to each shard for this table\n",
        "        rows_for_this_table = table_row_counts[table_name]\n",
        "        rows_per_shard_for_table = rows_for_this_table / N_values[i]\n",
        "        remaining_rows_for_table = rows_for_this_table % N_values[i]\n",
        "\n",
        "        # Store allocation information\n",
        "        table_shard_allocation[table_name] = {\n",
        "            'N_value': N_values[i],\n",
        "            'rows_per_shard': [int(rows_per_shard_for_table)] * N_values[i]\n",
        "        }\n",
        "\n",
        "        # Track shard index ranges for metadata\n",
        "        table_metadata[table_name] = {\n",
        "            'total_rows': table_row_count,\n",
        "            'total_shards': N_values[i],\n",
        "            'shards': []\n",
        "        }\n",
        "\n",
        "        # Allocate rows to each shard based on N_values for the table\n",
        "        current_row_start = 0\n",
        "        for j in range(N_values[i]):  # For the N_values of this table\n",
        "            # Check if we're at the last shard, allocate remaining rows if necessary\n",
        "            rows_to_allocate = int(rows_per_shard_for_table)  # Integer number of rows to allocate\n",
        "\n",
        "            # If there are remaining rows, distribute them one by one to the shards\n",
        "            if j < remaining_rows_for_table:\n",
        "                rows_to_allocate += 1  # Allocate one more row to this shard\n",
        "\n",
        "            # Calculate row range for this shard\n",
        "            current_row_end = current_row_start + rows_to_allocate - 1\n",
        "            table_metadata[table_name]['shards'].append({\n",
        "                'shard_index': current_shard_index,\n",
        "                'row_range': f\"{current_row_start}-{current_row_end}\",\n",
        "                'total_rows': rows_to_allocate\n",
        "            })\n",
        "\n",
        "            # Update the start row for the next shard\n",
        "            current_row_start = current_row_end + 1\n",
        "\n",
        "            # Add rows to the current shard\n",
        "            shard_data[current_shard_index].append((table_name, rows_to_allocate))\n",
        "            print(f\"Table: {table_name} | Shard {current_shard_index} | Rows Allocated: {rows_to_allocate}\")\n",
        "\n",
        "            # Move to the next shard index\n",
        "            current_shard_index += 1\n",
        "            if current_shard_index == shard_count:\n",
        "                break\n",
        "\n",
        "    # Step 6: Save sharded data into separate files\n",
        "    for shard_index, shard_rows in shard_data.items():\n",
        "        # For each shard, we will collect all rows assigned to it and store them in a DataFrame\n",
        "        shard_rows_data = []\n",
        "\n",
        "        for table_name, rows_to_allocate in shard_rows:\n",
        "            # Get the rows for the specific table and add them to the list\n",
        "            table_df = tables[table_names.index(table_name)]\n",
        "            rows = table_df.head(rows_to_allocate)  # Take only the required number of rows\n",
        "            shard_rows_data.append(rows)\n",
        "\n",
        "        # Concatenate all the rows from all tables into a single DataFrame for the shard\n",
        "        shard_df = pd.concat(shard_rows_data, ignore_index=True)\n",
        "\n",
        "        # Save the DataFrame to a CSV file for the shard\n",
        "        shard_file_path = os.path.join(output_folder, f'shard_{shard_index}.csv')\n",
        "        shard_df.to_csv(shard_file_path, index=False)\n",
        "        print(f\"Shard {shard_index} saved to {shard_file_path}\")\n",
        "\n",
        "    # Step 7: Write the metadata file (JSON)\n",
        "    metadata_file_path = os.path.join(output_folder, 'shard_metadata.json')\n",
        "\n",
        "    # Save metadata to JSON\n",
        "    with open(metadata_file_path, 'w') as json_file:\n",
        "        json.dump(table_metadata, json_file, indent=4)\n",
        "\n",
        "    print(f\"Metadata saved to {metadata_file_path}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "input_folder = '/content/sample_data'  # Folder where original CSV tables are located\n",
        "output_folder = '/content/sharded_database'  # Folder where sharded tables will be saved\n",
        "shard_count = 20  # Number of shards to split the tables into\n",
        "\n",
        "shard_database(input_folder=input_folder, output_folder=output_folder, shard_count=shard_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7-ho2SoNk5q",
        "outputId": "4b9a7768-1dfc-4025-8f30-67a31a84a004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Executing query: SELECT * FROM WDICountry WHERE Country Code = 'USA'\n",
            "Error executing query: 'Code'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Union\n",
        "import sqlparse\n",
        "from sqlparse.sql import Where, Comparison, Identifier, Token\n",
        "import re\n",
        "\n",
        "class DistributedQueryEngine:\n",
        "    def __init__(self, database_path: str, metadata_path: str, shard_metadata_path: str):\n",
        "        \"\"\"\n",
        "        Initialize the distributed query engine.\n",
        "\n",
        "        Args:\n",
        "            database_path: Path to the folder containing sharded CSV files\n",
        "            metadata_path: Path to the database schema metadata JSON file\n",
        "            shard_metadata_path: Path to the shard distribution metadata JSON file\n",
        "        \"\"\"\n",
        "        self.database_path = Path(database_path)\n",
        "        self.schema = self._load_json(metadata_path)\n",
        "        self.shard_metadata = self._load_json(shard_metadata_path)\n",
        "\n",
        "    def _load_json(self, path: str) -> Dict:\n",
        "        \"\"\"Load and parse a JSON file.\"\"\"\n",
        "        with open(path, 'r') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def _parse_where_clause(self, where_clause: Where) -> Dict[str, Any]:\n",
        "        \"\"\"Parse WHERE clause to extract conditions.\"\"\"\n",
        "        conditions = {}\n",
        "        if where_clause:\n",
        "            for token in where_clause.tokens:\n",
        "                if isinstance(token, Comparison):\n",
        "                    # Extract column name and value from comparison\n",
        "                    left = str(token.left).strip()\n",
        "                    right = str(token.right).strip().strip(\"'\").strip('\"')\n",
        "                    operator = str(token.token_next(0)[1])\n",
        "                    conditions[left] = {'value': right, 'operator': operator}\n",
        "        return conditions\n",
        "\n",
        "    def _identify_relevant_shards(self, table_name: str, conditions: Dict) -> List[int]:\n",
        "        \"\"\"Identify which shards need to be queried based on conditions.\"\"\"\n",
        "        if table_name not in self.shard_metadata:\n",
        "            raise ValueError(f\"Table {table_name} not found in shard metadata\")\n",
        "\n",
        "        # For now, return all shards - this could be optimized based on conditions\n",
        "        return [shard['shard_index'] for shard in self.shard_metadata[table_name]['shards']]\n",
        "\n",
        "    def _read_shard(self, shard_index: int) -> pd.DataFrame:\n",
        "        \"\"\"Read a specific shard file.\"\"\"\n",
        "        shard_path = self.database_path / f'shard_{shard_index}.csv'\n",
        "        return pd.read_csv(shard_path)\n",
        "\n",
        "    def _apply_conditions(self, df: pd.DataFrame, conditions: Dict) -> pd.DataFrame:\n",
        "        \"\"\"Apply WHERE conditions to the DataFrame.\"\"\"\n",
        "        for column, condition in conditions.items():\n",
        "            operator = condition['operator']\n",
        "            value = condition['value']\n",
        "\n",
        "            if operator == '=':\n",
        "                df = df[df[column] == value]\n",
        "            elif operator == '>':\n",
        "                df = df[df[column] > float(value)]\n",
        "            elif operator == '<':\n",
        "                df = df[df[column] < float(value)]\n",
        "            elif operator == '>=':\n",
        "                df = df[df[column] >= float(value)]\n",
        "            elif operator == '<=':\n",
        "                df = df[df[column] <= float(value)]\n",
        "\n",
        "        return df\n",
        "\n",
        "    def execute_query(self, query: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Execute a simplified SQL query across the distributed database.\n",
        "\n",
        "        Currently supports basic SELECT queries with WHERE clauses.\n",
        "\n",
        "        Args:\n",
        "            query: SQL query string (simplified format)\n",
        "\n",
        "        Returns:\n",
        "            pandas DataFrame with query results\n",
        "        \"\"\"\n",
        "        # Parse the SQL query\n",
        "        parsed = sqlparse.parse(query)[0]\n",
        "\n",
        "        # Extract table name\n",
        "        from_seen = False\n",
        "        table_name = None\n",
        "        for token in parsed.tokens:\n",
        "            if from_seen and isinstance(token, Identifier):\n",
        "                table_name = str(token).strip()\n",
        "                break\n",
        "            if str(token).upper() == 'FROM':\n",
        "                from_seen = True\n",
        "\n",
        "        if not table_name:\n",
        "            raise ValueError(\"Could not identify table name in query\")\n",
        "\n",
        "        # Extract WHERE conditions if present\n",
        "        where_clause = None\n",
        "        for token in parsed.tokens:\n",
        "            if isinstance(token, Where):\n",
        "                where_clause = token\n",
        "                break\n",
        "\n",
        "        conditions = self._parse_where_clause(where_clause) if where_clause else {}\n",
        "\n",
        "        # Identify relevant shards\n",
        "        relevant_shards = self._identify_relevant_shards(table_name, conditions)\n",
        "\n",
        "        # Execute query across shards and combine results\n",
        "        results = []\n",
        "        for shard_index in relevant_shards:\n",
        "            shard_df = self._read_shard(shard_index)\n",
        "\n",
        "            # Apply conditions\n",
        "            if conditions:\n",
        "                shard_df = self._apply_conditions(shard_df, conditions)\n",
        "\n",
        "            results.append(shard_df)\n",
        "\n",
        "        # Combine results\n",
        "        if results:\n",
        "            final_result = pd.concat(results, ignore_index=True)\n",
        "\n",
        "            # Extract selected columns\n",
        "            select_columns = []\n",
        "            select_seen = False\n",
        "            for token in parsed.tokens:\n",
        "                if select_seen and isinstance(token, Identifier):\n",
        "                    select_columns.extend([col.strip() for col in str(token).split(',')])\n",
        "                if str(token).upper() == 'SELECT':\n",
        "                    select_seen = True\n",
        "                elif str(token).upper() == 'FROM':\n",
        "                    break\n",
        "\n",
        "            if select_columns and select_columns != ['*']:\n",
        "                final_result = final_result[select_columns]\n",
        "\n",
        "            return final_result\n",
        "        else:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def get_table_schema(self, table_name: str) -> Dict:\n",
        "        \"\"\"Get the schema information for a specific table.\"\"\"\n",
        "        if table_name not in self.schema:\n",
        "            raise ValueError(f\"Table {table_name} not found in schema\")\n",
        "        return self.schema[table_name]\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the query engine\n",
        "    engine = DistributedQueryEngine(\n",
        "        database_path=\"/content/sharded_database\",\n",
        "        metadata_path=\"/content/database_metadata.json\",\n",
        "        shard_metadata_path=\"/content/sharded_database/shard_metadata.json\"\n",
        "    )\n",
        "\n",
        "    # Example queries\n",
        "    queries = [\n",
        "        \"SELECT * FROM WDICountry.csv WHERE Country_Code = 'USA'\"\n",
        "    ]\n",
        "\n",
        "    # Execute example queries\n",
        "    for query in queries:\n",
        "        try:\n",
        "            print(f\"\\nExecuting query: {query}\")\n",
        "            result = engine.execute_query(query)\n",
        "            print(f\"Results shape: {result.shape}\")\n",
        "            print(result.head())\n",
        "        except Exception as e:\n",
        "            print(f\"Error executing query: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezMWjpLrRvyL",
        "outputId": "c56d8c49-12ea-4e08-fca2-2b1ae09b3eb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Executing query: SELECT * FROM WDICountry WHERE Country_Code = 'USA'\n",
            "Country Code\n",
            "Results shape: (1, 31)\n",
            "  Country Code     Short Name     Table Name                 Long Name  \\\n",
            "0          USA  United States  United States  United States of America   \n",
            "\n",
            "  2-alpha code Currency Unit Special Notes         Region Income Group  \\\n",
            "0           US   U.S. dollar           NaN  North America  High income   \n",
            "\n",
            "  WB-2 code  ...    Government Accounting concept  \\\n",
            "0        US  ...  Consolidated central government   \n",
            "\n",
            "                     IMF data dissemination standard Latest population census  \\\n",
            "0  Special Data Dissemination Standard Plus (SDDS...          2020 (expected)   \n",
            "\n",
            "  Latest household survey Source of most recent Income and expenditure data  \\\n",
            "0                     NaN                    Labor force survey (LFS), 2016   \n",
            "\n",
            "  Vital registration complete  Latest agricultural census  \\\n",
            "0                         Yes                        2012   \n",
            "\n",
            "   Latest industrial data Latest trade data Latest water withdrawal data  \n",
            "0                  2011.0            2018.0                          NaN  \n",
            "\n",
            "[1 rows x 31 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Union\n",
        "import sqlparse\n",
        "from sqlparse.sql import Where, Comparison, Identifier, Token\n",
        "import re\n",
        "\n",
        "class DistributedQueryEngine:\n",
        "    def __init__(self, database_path: str, metadata_path: str, shard_metadata_path: str):\n",
        "        \"\"\"\n",
        "        Initialize the distributed query engine.\n",
        "\n",
        "        Args:\n",
        "            database_path: Path to the folder containing sharded CSV files\n",
        "            metadata_path: Path to the database schema metadata JSON file\n",
        "            shard_metadata_path: Path to the shard distribution metadata JSON file\n",
        "        \"\"\"\n",
        "        self.database_path = Path(database_path)\n",
        "        self.schema = self._load_json(metadata_path)\n",
        "        self.shard_metadata = self._load_json(shard_metadata_path)\n",
        "\n",
        "    def _load_json(self, path: str) -> Dict:\n",
        "        \"\"\"Load and parse a JSON file.\"\"\"\n",
        "        with open(path, 'r') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def _normalize_column_name(self, column_name: str) -> str:\n",
        "        \"\"\"Normalize column names (e.g., replace spaces with underscores).\"\"\"\n",
        "        return column_name.replace(\"_\", \" \")\n",
        "\n",
        "    def _parse_where_clause(self, where_clause: Where) -> Dict[str, Any]:\n",
        "        \"\"\"Parse WHERE clause to extract conditions.\"\"\"\n",
        "        conditions = {}\n",
        "        if where_clause:\n",
        "            for token in where_clause.tokens:\n",
        "                if isinstance(token, Comparison):\n",
        "                    # Extract column name and value from comparison\n",
        "                    left = str(token.left).strip()\n",
        "                    right = str(token.right).strip().strip(\"'\").strip('\"')\n",
        "                    operator = str(token.token_next(0)[1])\n",
        "\n",
        "                    # Normalize column name\n",
        "                    left_normalized = self._normalize_column_name(left)\n",
        "\n",
        "                    conditions[left_normalized] = {'value': right, 'operator': operator}\n",
        "        return conditions\n",
        "\n",
        "    def _identify_relevant_shards(self, table_name: str, conditions: Dict) -> List[int]:\n",
        "        \"\"\"Identify which shards need to be queried based on conditions.\"\"\"\n",
        "        if table_name not in self.shard_metadata:\n",
        "            raise ValueError(f\"Table {table_name} not found in shard metadata\")\n",
        "\n",
        "        # For now, return all shards - this could be optimized based on conditions\n",
        "        return [shard['shard_index'] for shard in self.shard_metadata[table_name]['shards']]\n",
        "\n",
        "    def _read_shard(self, shard_index: int) -> pd.DataFrame:\n",
        "        \"\"\"Read a specific shard file.\"\"\"\n",
        "        shard_path = self.database_path / f'shard_{shard_index}.csv'\n",
        "        return pd.read_csv(shard_path)\n",
        "\n",
        "    def _apply_conditions(self, df: pd.DataFrame, conditions: Dict) -> pd.DataFrame:\n",
        "        \"\"\"Apply WHERE conditions to the DataFrame.\"\"\"\n",
        "        for column, condition in conditions.items():\n",
        "            operator = condition['operator']\n",
        "            value = condition['value']\n",
        "\n",
        "            # Normalize column name for condition\n",
        "            column_normalized = self._normalize_column_name(column)\n",
        "\n",
        "            if operator == '=':\n",
        "                df = df[df[column_normalized] == value]\n",
        "            elif operator == '>':\n",
        "                df = df[df[column_normalized] > float(value)]\n",
        "            elif operator == '<':\n",
        "                df = df[df[column_normalized] < float(value)]\n",
        "            elif operator == '>=':\n",
        "                df = df[df[column_normalized] >= float(value)]\n",
        "            elif operator == '<=':\n",
        "                df = df[df[column_normalized] <= float(value)]\n",
        "\n",
        "        return df\n",
        "\n",
        "    def execute_query(self, query: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Execute a simplified SQL query across the distributed database.\n",
        "\n",
        "        Currently supports basic SELECT queries with WHERE clauses.\n",
        "\n",
        "        Args:\n",
        "            query: SQL query string (simplified format)\n",
        "\n",
        "        Returns:\n",
        "            pandas DataFrame with query results\n",
        "        \"\"\"\n",
        "        # Parse the SQL query\n",
        "        parsed = sqlparse.parse(query)[0]\n",
        "\n",
        "        # Extract table name\n",
        "        from_seen = False\n",
        "        table_name = None\n",
        "        for token in parsed.tokens:\n",
        "            if from_seen and isinstance(token, Identifier):\n",
        "                table_name = str(token).strip()\n",
        "                break\n",
        "            if str(token).upper() == 'FROM':\n",
        "                from_seen = True\n",
        "\n",
        "        if not table_name:\n",
        "            raise ValueError(\"Could not identify table name in query\")\n",
        "\n",
        "        # Extract WHERE conditions if present\n",
        "        where_clause = None\n",
        "        for token in parsed.tokens:\n",
        "            if isinstance(token, Where):\n",
        "                where_clause = token\n",
        "                break\n",
        "\n",
        "        conditions = self._parse_where_clause(where_clause) if where_clause else {}\n",
        "\n",
        "        # Identify relevant shards\n",
        "        relevant_shards = self._identify_relevant_shards(table_name, conditions)\n",
        "\n",
        "        # Execute query across shards and combine results\n",
        "        results = []\n",
        "        for shard_index in relevant_shards:\n",
        "            shard_df = self._read_shard(shard_index)\n",
        "\n",
        "            # Apply conditions\n",
        "            if conditions:\n",
        "                shard_df = self._apply_conditions(shard_df, conditions)\n",
        "\n",
        "            results.append(shard_df)\n",
        "\n",
        "        # Combine results\n",
        "        if results:\n",
        "            final_result = pd.concat(results, ignore_index=True)\n",
        "\n",
        "            # Extract selected columns\n",
        "            select_columns = []\n",
        "            select_seen = False\n",
        "            for token in parsed.tokens:\n",
        "                if select_seen and isinstance(token, Identifier):\n",
        "                    select_columns.extend([col.strip() for col in str(token).split(',')])\n",
        "                if str(token).upper() == 'SELECT':\n",
        "                    select_seen = True\n",
        "                elif str(token).upper() == 'FROM':\n",
        "                    break\n",
        "\n",
        "            if select_columns and select_columns != ['*']:\n",
        "                final_result = final_result[select_columns]\n",
        "\n",
        "            return final_result\n",
        "        else:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def get_table_schema(self, table_name: str) -> Dict:\n",
        "        \"\"\"Get the schema information for a specific table.\"\"\"\n",
        "        if table_name not in self.schema:\n",
        "            raise ValueError(f\"Table {table_name} not found in schema\")\n",
        "        return self.schema[table_name]\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the query engine\n",
        "    engine = DistributedQueryEngine(\n",
        "        database_path=\"/content/sharded_database\",\n",
        "        metadata_path=\"/content/database_metadata.json\",\n",
        "        shard_metadata_path=\"/content/sharded_database/shard_metadata.json\"\n",
        "    )\n",
        "\n",
        "    # Example queries\n",
        "    queries = [\n",
        "        \"SELECT * FROM WDICountry WHERE Country_Code = 'USA'\"\n",
        "    ]\n",
        "\n",
        "    # Execute example queries\n",
        "    for query in queries:\n",
        "        try:\n",
        "            print(f\"\\nExecuting query: {query}\")\n",
        "            result = engine.execute_query(query)\n",
        "            print(f\"Results shape: {result.shape}\")\n",
        "            print(result.head())\n",
        "        except Exception as e:\n",
        "            print(f\"Error executing query: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-mgcHHiazW0",
        "outputId": "ef21e6c5-98b3-41e9-97d1-fb3fe6b56eed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from flask import Flask, render_template, request, redirect, url_for, flash, jsonify\n",
        "from werkzeug.utils import secure_filename\n",
        "import shutil\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.config['UPLOAD_FOLDER'] = 'uploads/'\n",
        "app.config['ALLOWED_EXTENSIONS'] = {'zip', 'tar', 'tar.gz'}\n",
        "app.secret_key = 'your_secret_key'\n",
        "\n",
        "# Initialize the query engine (empty for now)\n",
        "engine = None\n",
        "\n",
        "\n",
        "def allowed_file(filename):\n",
        "    return '.' in filename and filename.rsplit('.', 1)[1] in app.config['ALLOWED_EXTENSIONS']\n",
        "\n",
        "\n",
        "def extract_folder(zip_file, folder_name):\n",
        "    \"\"\"Helper function to extract the contents of the zip file to a folder.\"\"\"\n",
        "    if not os.path.exists(os.path.join(app.config['UPLOAD_FOLDER'], folder_name)):\n",
        "        os.makedirs(os.path.join(app.config['UPLOAD_FOLDER'], folder_name))\n",
        "    shutil.unpack_archive(zip_file, os.path.join(app.config['UPLOAD_FOLDER'], folder_name))\n",
        "\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('upload.html')\n",
        "\n",
        "\n",
        "@app.route('/upload', methods=['POST'])\n",
        "def upload_database():\n",
        "    if 'file' not in request.files:\n",
        "        flash('No file part')\n",
        "        return redirect(request.url)\n",
        "\n",
        "    file = request.files['file']\n",
        "\n",
        "    if file.filename == '':\n",
        "        flash('No selected file')\n",
        "        return redirect(request.url)\n",
        "\n",
        "    if file and allowed_file(file.filename):\n",
        "        company_name = request.form['company_name']\n",
        "        filename = secure_filename(file.filename)\n",
        "\n",
        "        # Save the uploaded zip file\n",
        "        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n",
        "        file.save(file_path)\n",
        "\n",
        "        # Extract contents of the zip to the folder with company name\n",
        "        extract_folder(file_path, company_name)\n",
        "\n",
        "\n",
        "        flash(f\"Database for {company_name} uploaded successfully!\")\n",
        "        return redirect(url_for('execute_query'))\n",
        "\n",
        "    flash('Invalid file format. Only zip files are allowed.')\n",
        "    return redirect(request.url)\n",
        "\n",
        "\n",
        "@app.route('/execute_query', methods=['GET', 'POST'])\n",
        "def execute_query():\n",
        "    global engine\n",
        "\n",
        "    if engine is None:\n",
        "        return redirect(url_for('index'))\n",
        "\n",
        "    if request.method == 'POST':\n",
        "        query = request.form['query']\n",
        "\n",
        "        try:\n",
        "            result = engine.execute_query(query)\n",
        "            result_html = result.to_html(classes=\"table table-bordered table-striped\", index=False)\n",
        "            return render_template('execute_query.html', result_html=result_html, query=query)\n",
        "        except Exception as e:\n",
        "            flash(f\"Error executing query: {e}\")\n",
        "            return redirect(request.url)\n",
        "\n",
        "    return render_template('execute_query.html')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOSKkvStBptLZMdCkV8hhH/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}